{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9de444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import InferSentPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb4499",
   "metadata": {},
   "source": [
    "## Performance of Sentence Encoder\n",
    "\n",
    "We evaluate the encoder trained on SNLI dataset (33% random baseline accuracy) on SentEval benchmark.\n",
    "\n",
    "### SNLI and SentEval accuracies\n",
    "\n",
    "We report the SentEval micro and macro accuracies. Micro is defined as the average of the SentEval accuracies, and macro is defined as the average of the SenteEval accuracies weighted by the number of samples of the respective tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec964523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dim</th>\n",
       "      <th>SNLI-dev</th>\n",
       "      <th>SNLI-test</th>\n",
       "      <th>transfer-micro</th>\n",
       "      <th>transfer-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awe</td>\n",
       "      <td>300</td>\n",
       "      <td>65.6</td>\n",
       "      <td>65.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>78.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lstm</td>\n",
       "      <td>2048</td>\n",
       "      <td>80.4</td>\n",
       "      <td>79.8</td>\n",
       "      <td>79.2</td>\n",
       "      <td>76.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bilstm</td>\n",
       "      <td>4096</td>\n",
       "      <td>78.2</td>\n",
       "      <td>78.6</td>\n",
       "      <td>80.4</td>\n",
       "      <td>78.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bilstm-max</td>\n",
       "      <td>4096</td>\n",
       "      <td>84.1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>81.9</td>\n",
       "      <td>80.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model   dim  SNLI-dev  SNLI-test  transfer-micro  transfer-macro\n",
       "0         awe   300      65.6       65.9            81.0            78.2\n",
       "1        lstm  2048      80.4       79.8            79.2            76.7\n",
       "2      bilstm  4096      78.2       78.6            80.4            78.3\n",
       "3  bilstm-max  4096      84.1       84.0            81.9            80.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b07ea",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- Increasing the complexity of the model, Average Word Embedding (AWE) to biLSTM with max pooling, improves the performance on SNLI.\n",
    "\n",
    "\n",
    "- While being mediocre on SNLI, AWE performs very well on the transfer tasks of SentEval (see next table). We believe that is because half of the tasks are \"sentimen-related\" and positive-negative sentiments information can be obtained using bag-of-words approaches with good results.\n",
    "\n",
    "\n",
    "- Recurrent models perform badly compared to the AWE approach on SentEval. This might be because the sentences in SNLI are relatively short (mean of 14 words for premise and 8 for hypothesis but with a long tail so median should be even lower) which is a good setup for recurrent models which tend to forget.\n",
    "\n",
    "\n",
    "- LSTM performing better than biLSTM on SNLI seems to be an outlier. It could be that the biLSTM run was a bad one since the performance on the SentEval benchmark suggests that the embeddings generated by the biLSTM contain more information.\n",
    "\n",
    "\n",
    "- Using all the tokens hidden representations is beneficial for the performance of the recurrent models for both the SNLI dataset and the SentEval benchmark.\n",
    "\n",
    "\n",
    "- It is not clear whether the max pooling is the reason for the good performance of the 4th model because it also makes use of all the hidden representations compared to the LSTM and the biLSTM. Trying other pooling layers (average, weighted average, etc.) would be interesting to find out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a95cb0",
   "metadata": {},
   "source": [
    "### Detailed SentEval performances\n",
    "We present the detailed performance of the SentEval tasks. All the tasks report the test accuracy except for SICK-R and STS14 which report the mean Pearson coefficient.\n",
    "\n",
    "#### Sentiment related tasks\n",
    "- MR: movie review\n",
    "- CR: product review\n",
    "- SUBJ: subjectivity status\n",
    "- MPQA: opinion-polary\n",
    "- SST: sentiment analysis\n",
    "\n",
    "#### Semantic related tasks\n",
    "- TREC: question-type classification\n",
    "- MRPC: paraphrase detection\n",
    "- SICK-R: semantic textual similarity\n",
    "- SICK-E: natural language inference\n",
    "- STS14: semantic textual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff8d4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MR</th>\n",
       "      <th>CR</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>MPQA</th>\n",
       "      <th>SST</th>\n",
       "      <th>TREC</th>\n",
       "      <th>MRPC</th>\n",
       "      <th>SICK-R</th>\n",
       "      <th>SICK-E</th>\n",
       "      <th>STS14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awe</td>\n",
       "      <td>74.47</td>\n",
       "      <td>77.69</td>\n",
       "      <td>89.45</td>\n",
       "      <td>84.53</td>\n",
       "      <td>78.58</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71.48</td>\n",
       "      <td>0.797</td>\n",
       "      <td>78.16</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lstm</td>\n",
       "      <td>70.56</td>\n",
       "      <td>75.71</td>\n",
       "      <td>85.41</td>\n",
       "      <td>84.40</td>\n",
       "      <td>74.79</td>\n",
       "      <td>67.8</td>\n",
       "      <td>72.87</td>\n",
       "      <td>0.855</td>\n",
       "      <td>81.69</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bilstm</td>\n",
       "      <td>71.24</td>\n",
       "      <td>77.30</td>\n",
       "      <td>87.55</td>\n",
       "      <td>84.71</td>\n",
       "      <td>76.77</td>\n",
       "      <td>73.8</td>\n",
       "      <td>72.06</td>\n",
       "      <td>0.859</td>\n",
       "      <td>83.30</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bilstm-max</td>\n",
       "      <td>74.67</td>\n",
       "      <td>80.42</td>\n",
       "      <td>90.82</td>\n",
       "      <td>85.38</td>\n",
       "      <td>80.56</td>\n",
       "      <td>76.8</td>\n",
       "      <td>73.97</td>\n",
       "      <td>0.877</td>\n",
       "      <td>83.99</td>\n",
       "      <td>0.633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model     MR     CR   SUBJ   MPQA    SST  TREC   MRPC  SICK-R  SICK-E  \\\n",
       "0         awe  74.47  77.69  89.45  84.53  78.58  71.0  71.48   0.797   78.16   \n",
       "1        lstm  70.56  75.71  85.41  84.40  74.79  67.8  72.87   0.855   81.69   \n",
       "2      bilstm  71.24  77.30  87.55  84.71  76.77  73.8  72.06   0.859   83.30   \n",
       "3  bilstm-max  74.67  80.42  90.82  85.38  80.56  76.8  73.97   0.877   83.99   \n",
       "\n",
       "   STS14  \n",
       "0  0.501  \n",
       "1  0.575  \n",
       "2  0.549  \n",
       "3  0.633  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results_senteval.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821bd6e",
   "metadata": {},
   "source": [
    "### Comments\n",
    "- AWE performs better on sentiment-related tasks and worse on tasks for which the semantics of the sentences are more important compared to the recurrent models.\n",
    "\n",
    "\n",
    "- Using a biLSTM is better for most tasks compared to a simple LSTM. Which confirms that the bidirectionality of the representation provides more detailed information.\n",
    "\n",
    "\n",
    "- Using all the tokens hidden representations is beneficial for the performance of the recurrent models for all the tasks in the SentEval benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58527cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InferSent(\n",
      "  (embeddings): Embedding(36698, 300)\n",
      "  (encoder): MaxBiLSTMModel(\n",
      "    (lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=3, bias=True)\n",
      "    (2): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "prediction = InferSentPrediction(\"bilstm-max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0846896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"two dogs play.\"\n",
    "hypothesis = \"two animals plays together.\"\n",
    "prediction.predict(premise, hypothesis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
